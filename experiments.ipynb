{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3c7ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from network import *\n",
    "from framework import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088715dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "isles_data_root = '/home/madtoinou/pdm/data/federated/synthetic/'\n",
    "modality = 'Tmax'\n",
    "batch_size = 2\n",
    "#num_epochs = 300\n",
    "#learning_rate = 0.000932#lrs[0] #To comment in the loop\n",
    "#weighting_scheme = 'FEDAVG'\n",
    "#beta_val=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2daf175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients=[\"center1\", \"center2\", \"center4\"]\n",
    "#from SCAFFOLD manuscript, global_learning_rate should be = sqrt(#Samples sites)\n",
    "local_epoch, global_epoch = 2, 20\n",
    "#no sampling\n",
    "K=len(clients)\n",
    "\n",
    "local_lr, global_lr = 0.00932, 1.7 #np.sqrt(K)\n",
    "\n",
    "_, centers_data_loaders, all_test_loader, _ = dataPreprocessing(isles_data_root, modality, 4, 2)\n",
    "\n",
    "#move center 3 at the end of the dataloaders\n",
    "tmp = centers_data_loaders[2]\n",
    "centers_data_loaders[2]=centers_data_loaders[3]\n",
    "centers_data_loaders[3]=tmp\n",
    "\n",
    "options = {'K': K, 'l_epoch': local_epoch, 'B': batch_size, 'g_epoch': global_epoch, 'clients': clients,\n",
    "           'l_lr':local_lr, 'g_lr':global_lr, 'dataloader':centers_data_loaders}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989c812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** global_epoch: 1 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madtoinou/documents/venvs/tml/lib/python3.7/site-packages/monai/losses/dice.py:150: UserWarning: single channel prediction, `include_background=False` ignored.\n",
      "  warnings.warn(\"single channel prediction, `include_background=False` ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center1 loss: 0.929850161075592\n",
      "center2 loss: 0.9844788908958435\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 2 ***\n",
      "center1 loss: 0.926748514175415\n",
      "center2 loss: 0.9866360425949097\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 3 ***\n",
      "center1 loss: 0.9369733929634094\n",
      "center2 loss: 0.973732590675354\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 4 ***\n",
      "center1 loss: 0.9189035892486572\n",
      "center2 loss: 0.9476808309555054\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 5 ***\n",
      "center1 loss: 0.8083707690238953\n",
      "center2 loss: 0.9825140833854675\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 6 ***\n",
      "center1 loss: 0.9217368364334106\n",
      "center2 loss: 0.9492074847221375\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 7 ***\n",
      "center1 loss: 0.8111785650253296\n",
      "center2 loss: 0.9824970364570618\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 8 ***\n",
      "center1 loss: 0.9217018485069275\n",
      "center2 loss: 0.9493539333343506\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 9 ***\n",
      "center1 loss: 0.8116187453269958\n",
      "center2 loss: 0.9824921488761902\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 10 ***\n",
      "center1 loss: 0.9216716289520264\n",
      "center2 loss: 0.9491956830024719\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 11 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 12 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 13 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 14 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 15 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 16 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 17 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 18 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 19 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "*** global_epoch: 20 ***\n",
      "center1 loss: nan\n",
      "center2 loss: nan\n",
      "center4 loss: nan\n",
      "dice: 0.0\n"
     ]
    }
   ],
   "source": [
    "scaffold = Scaffold(options)\n",
    "hist_loss = scaffold.train_server(global_epoch, local_epoch, global_lr, local_lr)\n",
    "scaffold.test(all_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f454a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** global_epoch: 1 ***\n",
      "center1 loss: 0.8623128533363342\n",
      "center2 loss: 0.9571033120155334\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 2 ***\n",
      "center1 loss: 0.8366109132766724\n",
      "center2 loss: 0.944184422492981\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 3 ***\n",
      "center1 loss: 0.7991296052932739\n",
      "center2 loss: 0.9258918762207031\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 4 ***\n",
      "center1 loss: 0.7459497451782227\n",
      "center2 loss: 0.897691011428833\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 5 ***\n",
      "center1 loss: 0.689969539642334\n",
      "center2 loss: 0.8663777112960815\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 6 ***\n",
      "center1 loss: 0.5984511375427246\n",
      "center2 loss: 0.8303675055503845\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 7 ***\n",
      "center1 loss: 0.5137276649475098\n",
      "center2 loss: 0.7994442582130432\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 8 ***\n",
      "center1 loss: 0.4466356635093689\n",
      "center2 loss: 0.7846068739891052\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 9 ***\n",
      "center1 loss: 0.5080825090408325\n",
      "center2 loss: 0.7607886791229248\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 10 ***\n",
      "center1 loss: 0.41845703125\n",
      "center2 loss: 0.7551860213279724\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 11 ***\n",
      "center1 loss: 0.301239550113678\n",
      "center2 loss: 0.6786140203475952\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 12 ***\n",
      "center1 loss: 0.29201340675354004\n",
      "center2 loss: 0.736103892326355\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 13 ***\n",
      "center1 loss: 0.2705972194671631\n",
      "center2 loss: 0.7338231205940247\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 14 ***\n",
      "center1 loss: 0.6670370697975159\n",
      "center2 loss: 0.7031826972961426\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 15 ***\n",
      "center1 loss: 0.2883177399635315\n",
      "center2 loss: 0.7016969919204712\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 16 ***\n",
      "center1 loss: 0.3596898317337036\n",
      "center2 loss: 0.7294530868530273\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 17 ***\n",
      "center1 loss: 0.30954718589782715\n",
      "center2 loss: 0.6673866510391235\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 18 ***\n",
      "center1 loss: 0.6038182973861694\n",
      "center2 loss: 0.6920828819274902\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 19 ***\n",
      "center1 loss: 0.3338608741760254\n",
      "center2 loss: 0.6765214204788208\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 20 ***\n",
      "center1 loss: 0.3302212953567505\n",
      "center2 loss: 0.6914693117141724\n",
      "center4 loss: 1.0\n",
      "dice: 0.46654441952705383\n"
     ]
    }
   ],
   "source": [
    "lt=0\n",
    "for j in centers_data_loaders:\n",
    "    lt +=len(j[0])\n",
    "\n",
    "options.update({'trainloaders_lengths':lt,\n",
    "                 'beta_val':0.9})\n",
    "        \n",
    "options['weighting_scheme']=\"BETA\"\n",
    "fedbeta = FedAvg(options)\n",
    "hist_loss = fedbeta.train_server(global_epoch, local_epoch, global_lr, local_lr)\n",
    "fedbeta.test(all_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582f1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** global_epoch: 1 ***\n",
      "center1 loss: 0.8458237648010254\n",
      "center2 loss: 0.9466336369514465\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 2 ***\n",
      "center1 loss: 0.8017458915710449\n",
      "center2 loss: 0.9237979650497437\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 3 ***\n",
      "center1 loss: 0.731121838092804\n",
      "center2 loss: 0.8920515775680542\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 4 ***\n",
      "center1 loss: 0.6568475961685181\n",
      "center2 loss: 0.8489447832107544\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 5 ***\n",
      "center1 loss: 0.55265212059021\n",
      "center2 loss: 0.8136163353919983\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 6 ***\n",
      "center1 loss: 0.4395267367362976\n",
      "center2 loss: 0.7830955982208252\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 7 ***\n",
      "center1 loss: 0.3592684864997864\n",
      "center2 loss: 0.7576908469200134\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 8 ***\n",
      "center1 loss: 0.36083221435546875\n",
      "center2 loss: 0.7232249975204468\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 9 ***\n",
      "center1 loss: 0.261584997177124\n",
      "center2 loss: 0.754959225654602\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 10 ***\n",
      "center1 loss: 0.37530726194381714\n",
      "center2 loss: 0.7198556065559387\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 11 ***\n",
      "center1 loss: 0.2810131311416626\n",
      "center2 loss: 0.7302236557006836\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 12 ***\n",
      "center1 loss: 0.2884230613708496\n",
      "center2 loss: 0.7363361120223999\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 13 ***\n",
      "center1 loss: 0.7899424433708191\n",
      "center2 loss: 0.7379748821258545\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 14 ***\n",
      "center1 loss: 0.41067278385162354\n",
      "center2 loss: 0.7203209400177002\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 15 ***\n",
      "center1 loss: 0.36649101972579956\n",
      "center2 loss: 0.7411977052688599\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 16 ***\n",
      "center1 loss: 0.2728649973869324\n",
      "center2 loss: 0.7100317478179932\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 17 ***\n",
      "center1 loss: 0.36986851692199707\n",
      "center2 loss: 0.701082706451416\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 18 ***\n",
      "center1 loss: 0.6001567244529724\n",
      "center2 loss: 0.7253977656364441\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 19 ***\n",
      "center1 loss: 0.3193889856338501\n",
      "center2 loss: 0.6825556755065918\n",
      "center4 loss: 1.0\n",
      "*** global_epoch: 20 ***\n",
      "center1 loss: 0.30607837438583374\n",
      "center2 loss: 0.7268015146255493\n",
      "center4 loss: 1.0\n",
      "dice: 0.425891637802124\n"
     ]
    }
   ],
   "source": [
    "options['weighting_scheme']=\"FEDAVG\"\n",
    "fedavg = FedAvg(options)\n",
    "hist_loss = fedavg.train_server(global_epoch, local_epoch, global_lr, local_lr)\n",
    "fedavg.test(all_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a307122",
   "metadata": {},
   "source": [
    "- batch_size is limited by the maximum number of slice for one patient, due to the Monai Implementation.\n",
    "- we should be able to increase it and only be dependent on the number of patient?\n",
    "- batchsize of 3, with 2 patients with 2 scans : one patient might be picked twice and crash the dataloader\n",
    "\n",
    "- winner of the contest, using all the modalities 0.56 or 0.55 \n",
    "- with T-max centralized : 0.4\n",
    "- with T-max decentralized : 0.26\n",
    "- play with the learning_rate to see how the evolution of the loss evolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99456a",
   "metadata": {},
   "source": [
    "# Observations about the training\n",
    "The loss is not stable from one global epoch to the next\n",
    "## Only 3 sites\n",
    "- site 2 seems unstable; after 10 global epochs, the loss becomes nan value and this get propagated to the central model\n",
    "\n",
    "\n",
    "# Misc\n",
    "C variable(attribute is not used anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac12c5",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- clean OO implementation for Scaffold, FedAvg, Softmax and B-weighting\n",
    "- find a way to increase the batch size; these tiny batches are not satisfying. Too sensible to data augmentation techniques..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tml",
   "language": "python",
   "name": "tml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
