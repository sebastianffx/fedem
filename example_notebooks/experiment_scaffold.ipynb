{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1e0ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from utils_fedavg import get_optimizer\n",
    "from weighting_schemes import average_weights, average_weights_beta, average_weights_softmax\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import monai\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "from scipy.spatial import distance_matrix\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsChannelFirstD,\n",
    "    AddChannel,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandRotate90,\n",
    "    RandSpatialCrop,\n",
    "    ScaleIntensity,\n",
    "    EnsureType,\n",
    "    Resized\n",
    ")\n",
    "\n",
    "from monai.data import (\n",
    "    ArrayDataset, GridPatchDataset, create_test_image_3d, PatchIter)\n",
    "from monai.utils import first\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from natsort import natsorted\n",
    "import umap\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dee07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = 'scan' #laptop\n",
    "if LOCATION == 'scan':\n",
    "    isles_data_root = '/home/madtoinou/pdm/data/federated/synthetic/'\n",
    "    exp_root = '/home/madtoinou/pdm/data/'\n",
    "\n",
    "if LOCATION == 'laptop':\n",
    "    isles_data_root = '/data/ASAP/miccai22_data/isles/federated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb963580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams cell\n",
    "modality = 'Tmax'\n",
    "batch_size = 2\n",
    "num_epochs = 300\n",
    "learning_rate = 0.000932#lrs[0] #To comment in the loop\n",
    "weighting_scheme = 'FEDAVG'\n",
    "beta_val=0.9\n",
    "\n",
    "#creating the dataloader for 10 ISLES volumes using the T_max and the CBF\n",
    "#For cbf we are windowing 1-1024\n",
    "#For tmax we'll window 0-60\n",
    "#For CBV we'll window 0-200\n",
    "if modality =='CBF':\n",
    "    max_intensity = 1200\n",
    "if modality =='CBV':\n",
    "    max_intensity = 200\n",
    "if modality =='Tmax' or modality =='MTT':\n",
    "    max_intensity = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d7323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloaders\n",
    "\n",
    "#Data augmentation operations\n",
    "imtrans = Compose(\n",
    "    [   LoadImage(image_only=True),\n",
    "        #RandScaleIntensity( factors=0.1, prob=0.5),\n",
    "        ScaleIntensity(minv=0.0, maxv=max_intensity),\n",
    "        AddChannel(),\n",
    "        RandRotate90( prob=0.5, spatial_axes=[0, 1]),\n",
    "        RandSpatialCrop((224, 224,1), random_size=False),\n",
    "        EnsureType(),\n",
    "        #Resized\n",
    "    ]\n",
    ")\n",
    "\n",
    "segtrans = Compose(\n",
    "    [   LoadImage(image_only=True),\n",
    "        AddChannel(),\n",
    "        RandRotate90( prob=0.5, spatial_axes=[0, 1]),\n",
    "        RandSpatialCrop((224, 224,1), random_size=False),\n",
    "        EnsureType(),\n",
    "        #Resized\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "imtrans_neutral = Compose(\n",
    "    [   LoadImage(image_only=True),\n",
    "        #RandScaleIntensity( factors=0.1, prob=0.5),\n",
    "        ScaleIntensity(minv=0.0, maxv=max_intensity),\n",
    "        AddChannel(),\n",
    "        RandSpatialCrop((224, 224,1), random_size=False),\n",
    "        EnsureType(),\n",
    "        #Resized\n",
    "    ]\n",
    ")\n",
    "\n",
    "segtrans_neutral = Compose(\n",
    "    [   LoadImage(image_only=True),\n",
    "        AddChannel(),\n",
    "        RandSpatialCrop((224, 224,1), random_size=False),\n",
    "        EnsureType(),\n",
    "        #Resized\n",
    "    ]\n",
    ")\n",
    "\n",
    "imtrans_test = Compose(\n",
    "    [   LoadImage(image_only=True),\n",
    "        ScaleIntensity(minv=0.0, maxv=max_intensity),\n",
    "        AddChannel(),\n",
    "        #RandSpatialCrop((224, 224,1), random_size=False), In test we would like to process ALL slices\n",
    "        EnsureType(),\n",
    "        #Resized\n",
    "    ]\n",
    ")\n",
    "\n",
    "segtrans_test = Compose(\n",
    "    [   LoadImage(image_only=True),\n",
    "        AddChannel(),\n",
    "        #RandSpatialCrop((224, 224,1), random_size=False),\n",
    "        EnsureType(),\n",
    "        #Resized\n",
    "    ]\n",
    ")\n",
    "\n",
    "def get_train_valid_test_partitions(modality, isles_data_root, num_centers=4):\n",
    "    centers_partitions = [[] for i in range(num_centers)]\n",
    "    for center_num in range(1,num_centers+1):\n",
    "        center_paths_train  = sorted(glob(isles_data_root+'center'+str(center_num)+'/train'+'/**/*'+modality+'*/*.nii'))\n",
    "        center_paths_valid  = sorted(glob(isles_data_root+'center'+str(center_num)+'/valid'+'/**/*'+modality+'*/*.nii'))\n",
    "        center_paths_test   = sorted(glob(isles_data_root+'center'+str(center_num)+'/test'+'/**/*'+modality+'*/*.nii'))\n",
    "        center_lbl_paths_train  = sorted(glob(isles_data_root+'center'+str(center_num)+'/train'+'/**/*OT*/*nii'))\n",
    "        center_lbl_paths_valid  = sorted(glob(isles_data_root+'center'+str(center_num)+'/valid'+'/**/*OT*/*nii'))\n",
    "        center_lbl_paths_test  = sorted(glob(isles_data_root+'center'+str(center_num)+'/test'+'/**/*OT*/*nii'))\n",
    "        centers_partitions[center_num-1] = [[center_paths_train,center_paths_valid,center_paths_test],[center_lbl_paths_train,center_lbl_paths_valid,center_lbl_paths_test]]\n",
    "    return centers_partitions\n",
    "\n",
    "def center_dataloaders(partitions_paths_center, batch_size=2):#\n",
    "    center_ds_train = ArrayDataset(partitions_paths_center[0][0], imtrans, partitions_paths_center[1][0], segtrans)\n",
    "    center_train_loader   = torch.utils.data.DataLoader(\n",
    "        center_ds_train, batch_size=batch_size, num_workers=1, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    center_ds_valid = ArrayDataset(partitions_paths_center[0][1], imtrans, partitions_paths_center[1][1], segtrans)\n",
    "    center_valid_loader   = torch.utils.data.DataLoader(\n",
    "        center_ds_valid, batch_size=batch_size, num_workers=1, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    center_ds_test = ArrayDataset(partitions_paths_center[0][2], imtrans_test, partitions_paths_center[1][2], segtrans_test)\n",
    "    center_test_loader   = torch.utils.data.DataLoader(\n",
    "        center_ds_test, batch_size=batch_size, num_workers=1, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    return center_train_loader, center_valid_loader, center_test_loader\n",
    "\n",
    "def creater_dataloaders(modality, path, number_site, batch_size):\n",
    "    partitions_paths = get_train_valid_test_partitions(modality, path, number_site)\n",
    "\n",
    "    centers_data_loaders = []\n",
    "    for i in range(len(partitions_paths)):#Adding all the centers data loaders\n",
    "        centers_data_loaders.append(center_dataloaders(partitions_paths[i],batch_size))\n",
    "        \n",
    "    return partitions_paths, centers_data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a165e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_paths, centers_data_loaders = creater_dataloaders(modality, isles_data_root, 4, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2482530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-organization for validation and test\n",
    "partitions_test_imgs = [partitions_paths[i][0][2] for i in range(len(partitions_paths))]\n",
    "partitions_test_lbls = [partitions_paths[i][1][2] for i in range(len(partitions_paths))]\n",
    "\n",
    "partitions_valid_imgs = [partitions_paths[i][0][1] for i in range(len(partitions_paths))]\n",
    "partitions_valid_lbls = [partitions_paths[i][1][1] for i in range(len(partitions_paths))]\n",
    "\n",
    "#For selecting the model and testing in the heldout partition we collect the valid and test data from ALL centers\n",
    "all_ds_test = ArrayDataset([i for l in partitions_test_imgs for i in l],\n",
    "                            imtrans, [i for l in partitions_test_lbls for i in l],\n",
    "                            segtrans)\n",
    "all_test_loader   = torch.utils.data.DataLoader(\n",
    "    all_ds_test, batch_size=1, num_workers=1, pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "\n",
    "all_ds_valid = ArrayDataset([i for l in partitions_valid_imgs for i in l],\n",
    "                            imtrans, [i for l in partitions_valid_lbls for i in l],\n",
    "                            segtrans)\n",
    "all_valid_loader   = torch.utils.data.DataLoader(\n",
    "    all_ds_valid, batch_size=1, num_workers=1, pin_memory=torch.cuda.is_available()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f9096",
   "metadata": {},
   "source": [
    "### SCAFFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fa6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09236961",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCAFFOLD\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#optimizer\n",
    "class ScaffoldOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr, weight_decay):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super(ScaffoldOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, server_controls, client_controls, closure=None):\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p, c, ci in zip(group['params'], server_controls.values(), client_controls.values()):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                dp = p.grad.data + c.data - ci.data\n",
    "                p.data = p.data - dp.data * group['lr']\n",
    "\n",
    "#network present in each client\n",
    "class SCAF_unet(monai.networks.nets.UNet):\n",
    "    def __init__(self, spatial_dims, in_channels, out_channels, channels, strides, kernel_size, num_res_units, name, E, lr):\n",
    "        #call parent constructor\n",
    "        super(SCAF_unet, self).__init__(spatial_dims=spatial_dims,\n",
    "                                        in_channels=in_channels,\n",
    "                                        out_channels=out_channels, \n",
    "                                        channels=channels,\n",
    "                                        strides=strides,\n",
    "                                        kernel_size=kernel_size, \n",
    "                                        num_res_units=num_res_units)\n",
    "\n",
    "        self.name = name\n",
    "        #control variables for SCAFFOLD\n",
    "        self.E = E\n",
    "        self.lr = lr\n",
    "        self.control = {}\n",
    "        self.delta_control = {}\n",
    "        self.delta_y = {}\n",
    "\n",
    "class Scaffold:\n",
    "    def __init__(self, options):\n",
    "        self.C = options['C']\n",
    "        self.E = options['E']\n",
    "        self.B = options['B']\n",
    "        self.K = options['K']\n",
    "        self.r = options['r']\n",
    "\n",
    "        #save all clients dataloader\n",
    "        self.dataloaders = options['dataloader']\n",
    "        \n",
    "        #server model\n",
    "        self.nn = SCAF_unet(spatial_dims=2,\n",
    "                            in_channels=1,\n",
    "                            out_channels=1,\n",
    "                            channels=(16, 32, 64, 128),\n",
    "                            strides=(2, 2, 2),\n",
    "                            kernel_size = (3,3),\n",
    "                            num_res_units=2,\n",
    "                            name='server',\n",
    "                            E=options['E'],\n",
    "                            lr=options['lr']).to(device)\n",
    "        \n",
    "        for k, v in self.nn.named_parameters():\n",
    "            self.nn.control[k] = torch.zeros_like(v.data)\n",
    "            self.nn.delta_control[k] = torch.zeros_like(v.data)\n",
    "            self.nn.delta_y[k] = torch.zeros_like(v.data)\n",
    "        \n",
    "        #create clients\n",
    "        self.nns = []\n",
    "        for i in range(self.K):\n",
    "            temp = copy.deepcopy(self.nn)\n",
    "            temp.name = options['clients'][i]\n",
    "            temp.control = copy.deepcopy(self.nn.control)  # ci\n",
    "            temp.delta_control = copy.deepcopy(self.nn.delta_control)  # ci\n",
    "            temp.delta_y = copy.deepcopy(self.nn.delta_y)\n",
    "            temp.E = options['E']\n",
    "            self.nns.append(temp)\n",
    "\n",
    "    def train_server(self, epoch, learning_rate):\n",
    "        for t in range(self.r):\n",
    "            print('*** round', t + 1, '***')\n",
    "            \n",
    "            #skiping center 2 as only 1 scan is available\n",
    "            #index=[0,1,3]\n",
    "            \n",
    "            #center 2 in included to check if it overfits (expected)\n",
    "            index=[0,1,2,3]\n",
    "            \n",
    "            # dispatch\n",
    "            self.dispatch(index)\n",
    "            # local updating\n",
    "            self.client_update(index, epoch, learning_rate)\n",
    "            # aggregation\n",
    "            self.aggregation(index)\n",
    "\n",
    "        return self.nn\n",
    "\n",
    "    def aggregation(self, index):\n",
    "        s = 0.0\n",
    "        for j in index:\n",
    "            # normal\n",
    "            s += self.nns[j].len\n",
    "        # compute\n",
    "        x = {}\n",
    "        c = {}\n",
    "        # init\n",
    "        for k, v in self.nns[0].named_parameters():\n",
    "            x[k] = torch.zeros_like(v.data)\n",
    "            c[k] = torch.zeros_like(v.data)\n",
    "\n",
    "        for j in index:\n",
    "            for k, v in self.nns[j].named_parameters():\n",
    "                x[k] += self.nns[j].delta_y[k] / len(index)  # averaging\n",
    "                c[k] += self.nns[j].delta_control[k] / len(index)  # averaging\n",
    "\n",
    "        # update x and c\n",
    "        for k, v in self.nn.named_parameters():\n",
    "            v.data += x[k].data  # lr=1\n",
    "            self.nn.control[k].data += c[k].data * (len(index) / self.K)\n",
    "\n",
    "    def dispatch(self, index):\n",
    "        for j in index:\n",
    "            for old_params, new_params in zip(self.nns[j].parameters(), self.nn.parameters()):\n",
    "                old_params.data = new_params.data.clone()\n",
    "\n",
    "    def client_update(self, index, epoch, learning_rate):  # update nn\n",
    "        for k in index:\n",
    "            self.nns[k] = train(self.nns[k], self.nn, k, self.dataloaders[k][0], epoch, learning_rate)\n",
    "\n",
    "    def global_test(self, aggreg_dataloader_test):\n",
    "        model = self.nn\n",
    "        model.eval()\n",
    "        \n",
    "        #test the global model on each individual dataloader\n",
    "        for k, client in enumerate(self.nns):\n",
    "            print(\"testing on\", client.name, \"dataloader\")\n",
    "            test(model, self.dataloaders[k][2])\n",
    "        \n",
    "        #test the global model on aggregated dataloaders\n",
    "        print(\"testing on all the data\")\n",
    "        test(model, aggreg_dataloader_test)\n",
    "            \n",
    "        \n",
    "\n",
    "def train(ann, server, k, dataloader_train, epoch, learning_rate):\n",
    "    #train client to train mode\n",
    "    ann.train()\n",
    "    ann.len = len(dataloader_train)\n",
    "\n",
    "    print(\"training center\", k)\n",
    "    loss_function = monai.losses.DiceLoss(sigmoid=True)\n",
    "    loss = 0\n",
    "    x = copy.deepcopy(ann)\n",
    "    optimizer = ScaffoldOptimizer(ann.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        for batch_data in dataloader_train:\n",
    "            inputs, labels = batch_data[0][:,:,:,:,0].to(device), batch_data[1][:,:,:,:,0].to(device)\n",
    "            y_pred = ann(inputs)\n",
    "            loss = loss_function(y_pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(server.control, ann.control) #performing SGD on the control variables\n",
    "        print(\"epoch\", epoch, \":\", loss.item())\n",
    "    # update c\n",
    "    # c+ <- ci - c + 1/(E * lr) * (x-yi)\n",
    "    # save ann\n",
    "    temp = {}\n",
    "    for k, v in ann.named_parameters():\n",
    "        temp[k] = v.data.clone()\n",
    "    for k, v in x.named_parameters():\n",
    "        ann.control[k] = ann.control[k] - server.control[k] + (v.data - temp[k]) / (ann.E * ann.lr)\n",
    "        ann.delta_y[k] = temp[k] - v.data\n",
    "        ann.delta_control[k] = ann.control[k] - x.control[k]\n",
    "    return ann\n",
    "\n",
    "def test(ann, dataloader_test):\n",
    "    ann.eval()\n",
    "    pred = []\n",
    "    y = []\n",
    "    for test_data in dataloader_test:\n",
    "        with torch.no_grad():\n",
    "            test_img, test_label = test_data[0].to(device), test_data[1].to(device)\n",
    "            \n",
    "            test_pred = ann(test_img[:,:,:,:,0])\n",
    "            #WHY?\n",
    "            test_pred =  test_pred>0.5 #This assumes one slice in the last dim\n",
    "            \n",
    "            dice_metric(y_pred=test_pred, y=test_label[:,:,:,:,0])\n",
    "            \n",
    "    # aggregate the final mean dice result\n",
    "    metric = dice_metric.aggregate().item()\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()\n",
    "    print('dice:', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "302ae877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** round 1 ***\n",
      "training center 0\n",
      "epoch 0 : 0.9144351482391357\n",
      "epoch 1 : 0.9094823002815247\n",
      "epoch 2 : 0.904748797416687\n",
      "epoch 3 : 0.900504469871521\n",
      "epoch 4 : 0.8967984318733215\n",
      "epoch 5 : 0.893869936466217\n",
      "epoch 6 : 0.8916093707084656\n",
      "epoch 7 : 0.8898556232452393\n",
      "epoch 8 : 0.888401985168457\n",
      "epoch 9 : 0.8871596455574036\n",
      "training center 1\n",
      "epoch 0 : 0.9948839545249939\n",
      "epoch 1 : 0.9943012595176697\n",
      "epoch 2 : 0.9939478635787964\n",
      "epoch 3 : 0.9936786890029907\n",
      "epoch 4 : 0.9934483170509338\n",
      "epoch 5 : 0.99323970079422\n",
      "epoch 6 : 0.9930410385131836\n",
      "epoch 7 : 0.9928483963012695\n",
      "epoch 8 : 0.9926561117172241\n",
      "epoch 9 : 0.9924585223197937\n",
      "training center 2\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "training center 3\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "*** round 2 ***\n",
      "training center 0\n",
      "epoch 0 : 0.8871600031852722\n",
      "epoch 1 : 0.8852335810661316\n",
      "epoch 2 : 0.8836990594863892\n",
      "epoch 3 : 0.882411777973175\n",
      "epoch 4 : 0.8813086152076721\n",
      "epoch 5 : 0.8803206086158752\n",
      "epoch 6 : 0.8794131875038147\n",
      "epoch 7 : 0.8785626888275146\n",
      "epoch 8 : 0.8777506351470947\n",
      "epoch 9 : 0.8769727349281311\n",
      "training center 1\n",
      "epoch 0 : 0.9940264225006104\n",
      "epoch 1 : 0.9937846660614014\n",
      "epoch 2 : 0.9936105012893677\n",
      "epoch 3 : 0.9934639930725098\n",
      "epoch 4 : 0.9933372735977173\n",
      "epoch 5 : 0.9932226538658142\n",
      "epoch 6 : 0.9931148290634155\n",
      "epoch 7 : 0.9930092692375183\n",
      "epoch 8 : 0.9929038286209106\n",
      "epoch 9 : 0.9927974343299866\n",
      "training center 2\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "training center 3\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "*** round 3 ***\n",
      "training center 0\n",
      "epoch 0 : 0.8776039481163025\n",
      "epoch 1 : 0.8767762184143066\n",
      "epoch 2 : 0.8759821653366089\n",
      "epoch 3 : 0.8752210736274719\n",
      "epoch 4 : 0.8744889497756958\n",
      "epoch 5 : 0.8737741708755493\n",
      "epoch 6 : 0.8730756640434265\n",
      "epoch 7 : 0.8723874688148499\n",
      "epoch 8 : 0.8717052936553955\n",
      "epoch 9 : 0.8710299134254456\n",
      "training center 1\n",
      "epoch 0 : 0.9935105443000793\n",
      "epoch 1 : 0.9933934211730957\n",
      "epoch 2 : 0.9932981729507446\n",
      "epoch 3 : 0.9932166337966919\n",
      "epoch 4 : 0.9931430816650391\n",
      "epoch 5 : 0.9930732250213623\n",
      "epoch 6 : 0.9930043816566467\n",
      "epoch 7 : 0.9929371476173401\n",
      "epoch 8 : 0.9928696751594543\n",
      "epoch 9 : 0.9928000569343567\n",
      "training center 2\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "training center 3\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "*** round 4 ***\n",
      "training center 0\n",
      "epoch 0 : 0.8721489906311035\n",
      "epoch 1 : 0.871511697769165\n",
      "epoch 2 : 0.8708770871162415\n",
      "epoch 3 : 0.8702448010444641\n",
      "epoch 4 : 0.8696149587631226\n",
      "epoch 5 : 0.8689854145050049\n",
      "epoch 6 : 0.8683546781539917\n",
      "epoch 7 : 0.8677202463150024\n",
      "epoch 8 : 0.8670827746391296\n",
      "epoch 9 : 0.8664414882659912\n",
      "training center 1\n",
      "epoch 0 : 0.9931094646453857\n",
      "epoch 1 : 0.9930528998374939\n",
      "epoch 2 : 0.9930005669593811\n",
      "epoch 3 : 0.9929513335227966\n",
      "epoch 4 : 0.9929033517837524\n",
      "epoch 5 : 0.9928545951843262\n",
      "epoch 6 : 0.9928045272827148\n",
      "epoch 7 : 0.9927530884742737\n",
      "epoch 8 : 0.9926993250846863\n",
      "epoch 9 : 0.9926429986953735\n",
      "training center 2\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "training center 3\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "*** round 5 ***\n",
      "training center 0\n",
      "epoch 0 : 0.867729663848877\n",
      "epoch 1 : 0.8671404123306274\n",
      "epoch 2 : 0.8665480017662048\n",
      "epoch 3 : 0.8659530282020569\n",
      "epoch 4 : 0.865353524684906\n",
      "epoch 5 : 0.8647487163543701\n",
      "epoch 6 : 0.8641386032104492\n",
      "epoch 7 : 0.8635225296020508\n",
      "epoch 8 : 0.8629001379013062\n",
      "epoch 9 : 0.8622721433639526\n",
      "training center 1\n",
      "epoch 0 : 0.9927753210067749\n",
      "epoch 1 : 0.9927422404289246\n",
      "epoch 2 : 0.9927072525024414\n",
      "epoch 3 : 0.9926702976226807\n",
      "epoch 4 : 0.9926308989524841\n",
      "epoch 5 : 0.9925891757011414\n",
      "epoch 6 : 0.9925448298454285\n",
      "epoch 7 : 0.9924971461296082\n",
      "epoch 8 : 0.9924463629722595\n",
      "epoch 9 : 0.9923931360244751\n",
      "training center 2\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n",
      "training center 3\n",
      "epoch 0 : 1.0\n",
      "epoch 1 : 1.0\n",
      "epoch 2 : 1.0\n",
      "epoch 3 : 1.0\n",
      "epoch 4 : 1.0\n",
      "epoch 5 : 1.0\n",
      "epoch 6 : 1.0\n",
      "epoch 7 : 1.0\n",
      "epoch 8 : 1.0\n",
      "epoch 9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    clients=[\"center1\", \"center2\", \"center3\", \"center4\"]\n",
    "    C, E, B, r = 0.8, num_epochs, batch_size, 5\n",
    "    #no sampling\n",
    "    K=len(clients)\n",
    "    learning_rate = 0.05\n",
    "    local_epoch=10\n",
    "    \n",
    "    _, centers_data_loaders = creater_dataloaders(modality, isles_data_root, 4, batch_size)\n",
    "    \n",
    "    options = {'K': K, 'C': C, 'E': E, 'B': B, 'r': r, 'clients': clients,\n",
    "               'lr':learning_rate, 'dataloader':centers_data_loaders}\n",
    "\n",
    "    scaffold = Scaffold(options)\n",
    "    scaffold.train_server(local_epoch, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6035999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on center1 dataloader\n",
      "dice: 0.10764572769403458\n",
      "testing on center2 dataloader\n",
      "dice: 0.3541662395000458\n",
      "testing on center3 dataloader\n",
      "dice: 0.0\n",
      "testing on center4 dataloader\n",
      "dice: 0.0\n",
      "testing on all the data\n",
      "dice: 0.3092420995235443\n"
     ]
    }
   ],
   "source": [
    "scaffold.global_test(all_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09834963",
   "metadata": {},
   "source": [
    "- batch_size is limited by the maximum number of slice for one patient, due to the Monai Implementation.\n",
    "- we should be able to increase it and only be dependent on the number of patient?\n",
    "- batchsize of 3, with 2 patients with 2 scans : one patient might be picked twice and crash the dataloader\n",
    "\n",
    "- winner of the contest, using all the modalities 0.56 or 0.55 \n",
    "- with T-max centralized : 0.4\n",
    "- with T-max decentralized : 0.26\n",
    "- play with the learning_rate to see how the evolution of the loss evolve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tml",
   "language": "python",
   "name": "tml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
