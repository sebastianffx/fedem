{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba59b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51bf4818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from network import *\n",
    "from framework import *\n",
    "from preprocessing import dataPreprocessing\n",
    "##Networks\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6515c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isles_data_root = '/str/data/ASAP/miccai22_data/isles/federated/'\n",
    "exp_root = '/home/otarola/miccai22/fedem/'\n",
    "modality = 'Tmax'\n",
    "batch_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b83ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients=[\"center1\", \"center2\", \"center4\"]\n",
    "#from SCAFFOLD manuscript, global_learning_rate should be = sqrt(#Samples sites)\n",
    "local_epoch, global_epoch = 2, 20\n",
    "#no sampling\n",
    "K=len(clients)\n",
    "\n",
    "local_lr, global_lr = 0.00932, 1.7 #np.sqrt(K)\n",
    "\n",
    "_, centers_data_loaders, all_test_loader, _ = dataPreprocessing(isles_data_root, modality, 4, 2)\n",
    "\n",
    "#move center 3 at the end of the dataloaders\n",
    "tmp = centers_data_loaders[2]\n",
    "centers_data_loaders[2]=centers_data_loaders[3]\n",
    "centers_data_loaders[3]=tmp\n",
    "\n",
    "options = {'K': K, 'l_epoch': local_epoch, 'B': batch_size, 'g_epoch': global_epoch, 'clients': clients,\n",
    "           'l_lr':local_lr, 'g_lr':global_lr, 'dataloader':centers_data_loaders, 'suffix': 'FedRod', \n",
    "           'scaffold_controls': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23b46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network present in each client\n",
    "class UNet_custom(monai.networks.nets.UNet):\n",
    "    def __init__(self, spatial_dims, in_channels, out_channels, channels,\n",
    "                 strides, kernel_size, num_res_units, name, scaff=False, fed_rod=False):\n",
    "        #call parent constructor\n",
    "        super(UNet_custom, self).__init__(spatial_dims=spatial_dims,\n",
    "                                          in_channels=in_channels,\n",
    "                                          out_channels=out_channels, \n",
    "                                          channels=channels,\n",
    "                                          strides=strides,\n",
    "                                          kernel_size=kernel_size, \n",
    "                                          num_res_units=num_res_units)\n",
    "\n",
    "        self.name = name\n",
    "        if scaff:\n",
    "            #control variables for SCAFFOLD\n",
    "            self.control = {}\n",
    "            self.delta_control = {}\n",
    "            self.delta_y = {}\n",
    "        if fed_rod:\n",
    "            #Unet params sets for FedRod\n",
    "            self.encoder_generic = {}\n",
    "            self.decoder_generic = {}\n",
    "            self.decoder_personalized = {}\n",
    "            if options['scaffold_controls']:\n",
    "                #In case we want FedRod + Scaffold controls\n",
    "                self.control = {}\n",
    "                self.delta_control = {}\n",
    "                self.delta_y = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94801e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22419a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedRod(Fedem):\n",
    "    def __init__(self, options):\n",
    "        super(FedRod, self).__init__(options)\n",
    "        self.writer = SummaryWriter(f\"runs/llr{options['l_lr']}_glr{options['g_lr']}_le{options['l_epoch']}_ge{options['g_epoch']}_{options['K']}sites_\"+\"FEDROD\"+options['suffix'])\n",
    "        self.K = options['K']\n",
    "        self.name_encoder_layers = [\"model.0\", \"model.1.submodule.0\", \"model.1.submodule.1.submodule.2.0\",\n",
    "                                    \"model.1.submodule.1.submodule.0\", \"model.1.submodule.1.submodule.1\"]\n",
    "        \n",
    "        self.name_decoder_layers = name_decoder_layers = ['model.1.submodule.1.submodule.2.1',\n",
    "                                                          'model.1.submodule.2', 'model.2']\n",
    "\n",
    "        #server model\n",
    "        self.nn = UNet_custom(spatial_dims=2,\n",
    "                             in_channels=1,\n",
    "                             out_channels=1,\n",
    "                             channels=(16, 32, 64, 128),\n",
    "                             strides=(2, 2, 2),\n",
    "                             kernel_size = (3,3),\n",
    "                             num_res_units=2,\n",
    "                             name='server',\n",
    "                             scaff=False,\n",
    "                             fed_rod=True).to(device)\n",
    "        \n",
    "        #Global encoder - decoder (inlcuding personalized) layers init\n",
    "        for k, v in self.nn.named_parameters():\n",
    "            for enc_layer in self.name_encoder_layers:\n",
    "                if enc_layer in k:\n",
    "                    self.nn.encoder_generic[k] = v.data\n",
    "            for dec_layer in self.name_decoder_layers:\n",
    "                if dec_layer in k:\n",
    "                    self.nn.decoder_generic[k] = v.data\n",
    "                    self.nn.decoder_personalized[k] = v.data\n",
    "                    \n",
    "        #print(self.nn.decoder_generic)\n",
    "        #clients of the federation\n",
    "        self.nns = []\n",
    "        for i in range(len(options['clients'])):\n",
    "            temp = copy.deepcopy(self.nn)\n",
    "            temp.name = options['clients'][i]\n",
    "            temp.encoder_generic = copy.deepcopy(self.nn.encoder_generic)\n",
    "            temp.decoder_generic = copy.deepcopy(self.nn.decoder_generic)\n",
    "            temp.decoder_personalized = copy.deepcopy(self.nn.decoder_personalized)            \n",
    "            self.nns.append(temp)\n",
    "            \n",
    "    \n",
    "\n",
    "    def aggregation(self, index, global_lr, **kwargs):\n",
    "        s = 0.0\n",
    "        for j in index:\n",
    "            # normal\n",
    "            s += self.nns[j].len\n",
    "                \n",
    "        # Agregating the generic encoder from clients encoders\n",
    "        for j in index:\n",
    "            for k, v in self.nn.named_parameters():\n",
    "                for enc_layer in self.name_encoder_layers:\n",
    "                    if enc_layer in k:\n",
    "                        v.data += self.nns[j].encoder_generic[k]  / len(index) #check other weightings here\n",
    "\n",
    "        # Agregating the generic decoder from clients decoders\n",
    "        for j in index:\n",
    "            for k, v in self.nn.named_parameters():\n",
    "                for dec_layer in self.name_decoder_layers:\n",
    "                    if dec_layer in k:\n",
    "                        v.data += self.nns[j].decoder_generic[k]  / len(index)  #check other weightings here\n",
    "\n",
    "\n",
    "    def train(self, ann, dataloader_train, local_epoch, local_lr):\n",
    "        #First the generic encoder-decoder are updated       \n",
    "        ann.train()\n",
    "        ann.len = len(dataloader_train)\n",
    "                \n",
    "        x = copy.deepcopy(ann)\n",
    "        loss_function = monai.losses.DiceLoss(sigmoid=True,include_background=False)\n",
    "        #One option is to set here the weights to 0 before the optimizer receives the parametets\n",
    "        optimizer = torch.optim.Adam(ann.parameters(), lr=local_lr)\n",
    "\n",
    "        for epoch in range(local_epoch):\n",
    "            for batch_data in dataloader_train:\n",
    "                #Optimization of the Generic path here\n",
    "                inputs, labels = batch_data[0][:,:,:,:,0].to(device), batch_data[1][:,:,:,:,0].to(device)\n",
    "                y_pred = ann(inputs)\n",
    "                loss = loss_function(y_pred, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step(self.nn.control, ann.control) #performing SGD on the control variables\n",
    "                \n",
    "                #Optimization of the Personalized path here. No gradient derivation w.r.t. generic enc. and dec.\n",
    "                optimizer.zero_grad()\n",
    "                y_pred_personalized = ann_two_branch(inputs)\n",
    "                loss = loss_function(y_pred, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step(self.nn.control, ann.control) #performing SGD on the control variables\n",
    "                            \n",
    "        # update c\n",
    "        # c+ <- ci - c + 1/(E * lr) * (x-yi)\n",
    "        temp = {}\n",
    "        for k, v in ann.named_parameters():\n",
    "            temp[k] = v.data.clone()\n",
    "        for k, v in x.named_parameters():\n",
    "            ann.control[k] = ann.control[k] - self.nn.control[k] + (v.data - temp[k]) / (local_epoch * local_lr)\n",
    "            ann.delta_y[k] = temp[k] - v.data\n",
    "            ann.delta_control[k] = ann.control[k] - x.control[k]\n",
    "        return theta_m, psi_m, loss.item()\n",
    "\n",
    "    def global_test(self, aggreg_dataloader_test):\n",
    "        model = self.nn\n",
    "        model.eval()\n",
    "        \n",
    "        #test the global model on each individual dataloader\n",
    "        for k, client in enumerate(self.nns):\n",
    "            print(\"testing on\", client.name, \"dataloader\")\n",
    "            test(model, self.dataloaders[k][2])\n",
    "        \n",
    "        #test the global model on aggregated dataloaders\n",
    "        print(\"testing on all the data\")\n",
    "        test(model, aggreg_dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4eca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_rod = FedRod(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb58ddfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of UNet_custom(\n",
       "  (model): Sequential(\n",
       "    (0): ResidualUnit(\n",
       "      (conv): Sequential(\n",
       "        (unit0): Convolution(\n",
       "          (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "        (unit1): Convolution(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): SkipConnection(\n",
       "      (submodule): Sequential(\n",
       "        (0): ResidualUnit(\n",
       "          (conv): Sequential(\n",
       "            (unit0): Convolution(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (unit1): Convolution(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): SkipConnection(\n",
       "          (submodule): Sequential(\n",
       "            (0): ResidualUnit(\n",
       "              (conv): Sequential(\n",
       "                (unit0): Convolution(\n",
       "                  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "                (unit1): Convolution(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (residual): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "            (1): SkipConnection(\n",
       "              (submodule): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                  (unit1): Convolution(\n",
       "                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Convolution(\n",
       "                (conv): ConvTranspose2d(192, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Convolution(\n",
       "            (conv): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (adn): ADN(\n",
       "              (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              (D): Dropout(p=0.0, inplace=False)\n",
       "              (A): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (conv): Sequential(\n",
       "              (unit0): Convolution(\n",
       "                (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (residual): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualUnit(\n",
       "        (conv): Sequential(\n",
       "          (unit0): Convolution(\n",
       "            (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (residual): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed_rod.nn.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8e1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = fed_rod.nn(torch.zeros([1, 1, 56, 224]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6493b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in fed_rod.nn.named_parameters():\n",
    "#    if \"model.1.submodule.1.submodule.1\" in k or \"model.0\" in k:\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "818d83f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "name_encoder_layers = [\"model.0\", \"model.1.submodule.0\", \"model.1.submodule.1.submodule.0\",\"model.1.submodule.1.submodule.1\"]\n",
    "\n",
    "for k,v in fed_rod.nn.named_parameters():\n",
    "    for enc_layer_name in name_encoder_layers:\n",
    "        if enc_layer_name in k:\n",
    "            print(k)\n",
    "            print(v.shape)\n",
    "            print(\"=====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92466035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = copy.deepcopy(fed_rod.nn)\n",
    "model2 = copy.deepcopy(fed_rod.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d6462f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet_custom(\n",
       "  (model): Sequential(\n",
       "    (0): ResidualUnit(\n",
       "      (conv): Sequential(\n",
       "        (unit0): Convolution(\n",
       "          (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "        (unit1): Convolution(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): SkipConnection(\n",
       "      (submodule): Sequential(\n",
       "        (0): ResidualUnit(\n",
       "          (conv): Sequential(\n",
       "            (unit0): Convolution(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (unit1): Convolution(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): SkipConnection(\n",
       "          (submodule): Sequential(\n",
       "            (0): ResidualUnit(\n",
       "              (conv): Sequential(\n",
       "                (unit0): Convolution(\n",
       "                  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "                (unit1): Convolution(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (residual): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "            (1): SkipConnection(\n",
       "              (submodule): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                  (unit1): Convolution(\n",
       "                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Convolution(\n",
       "                (conv): ConvTranspose2d(192, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Convolution(\n",
       "            (conv): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (adn): ADN(\n",
       "              (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              (D): Dropout(p=0.0, inplace=False)\n",
       "              (A): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (conv): Sequential(\n",
       "              (unit0): Convolution(\n",
       "                (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (residual): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualUnit(\n",
       "        (conv): Sequential(\n",
       "          (unit0): Convolution(\n",
       "            (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (residual): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed_rod.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd45326",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_encoder_layers = [\"model.0\", \"model.1.submodule.0\", \"model.1.submodule.1.submodule.2.0\",\n",
    "                            \"model.1.submodule.1.submodule.0\", \"model.1.submodule.1.submodule.1\"]\n",
    "\n",
    "name_decoder_layers = name_decoder_layers = ['model.1.submodule.1.submodule.2.1',\n",
    "                                                  'model.1.submodule.2', 'model.2']\n",
    "\n",
    "\n",
    "\n",
    "for k,v in model2.named_parameters():\n",
    "    for enc_layer_name in name_encoder_layers:\n",
    "        if enc_layer_name in k:\n",
    "            v.data = torch.zeros(v.shape).to(device)\n",
    "            v.requires_grad = False\n",
    "\n",
    "for k,v in model2.named_parameters():\n",
    "    for dec_layer_name in name_decoder_layers:\n",
    "        if dec_layer_name in k:\n",
    "            v.data = torch.ones(v.shape).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cef578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(1.3818, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.1102, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(1.5985, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(4.1116, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.7261, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(-1.9756, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(-0.2924, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(0.5780, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(-0.7115, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "tensor(0.4316, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(-5.1193, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "tensor(5.9209, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0.0351, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(2.7920, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "tensor(-0.5327, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "tensor(-0.9808, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0.0592, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "tensor(3.4588, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(-0.3010, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "tensor(5.5608, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "tensor(0.4023, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "tensor(2.4038, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(-0.1316, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(0.0906, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.0295, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "tensor(-2.0883, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.0182, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(-3.0485, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.1394, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "tensor(-0.3623, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(-0.2178, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor(-0.6678, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(-0.2681, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in fed_rod.nn.named_parameters():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(v.sum())    \n",
    "    print(\"=====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "011a1f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(1.3818, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.1102, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(1.5985, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(4.1116, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.7261, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(-1.9756, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(-0.2924, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(0.5780, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(-0.7115, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "tensor(0.4316, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(-5.1193, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "tensor(5.9209, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0.0351, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(2.7920, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "tensor(-0.5327, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "tensor(-0.9808, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0.0592, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "tensor(3.4588, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(-0.3010, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "tensor(5.5608, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "tensor(0.4023, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "tensor(2.4038, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(-0.1316, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(0.0906, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.0295, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "tensor(-2.0883, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.0182, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(-3.0485, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.1394, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "tensor(-0.3623, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(-0.2178, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor(-0.6678, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(-0.2681, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in model1.named_parameters():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(v.sum())    \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8f61e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_decoder_layers = ['model.1.submodule.1.submodule.2.1','model.1.submodule.2', 'model.2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13443ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0')\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(9216., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(32., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "tensor(9216., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(16., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(2304., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(16., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "tensor(288., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor(9., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in model2.named_parameters():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(v.sum())    \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738b40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
