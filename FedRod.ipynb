{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba59b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "#from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51bf4818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from network import *\n",
    "from framework import *\n",
    "from preprocessing import dataPreprocessing\n",
    "##Networks\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6515c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isles_data_root = '/Users/sebastianotalora/work/postdoc/data/ISLES/federated/'\n",
    "exp_root = '/Users/sebastianotalora/work/tmi/fedem'\n",
    "modality = 'Tmax'\n",
    "batch_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b83ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients=[\"center1\", \"center2\", \"center4\"]\n",
    "#from SCAFFOLD manuscript, global_learning_rate should be = sqrt(#Samples sites)\n",
    "local_epoch, global_epoch = 1, 20\n",
    "#no sampling\n",
    "K=len(clients)\n",
    "\n",
    "local_lr, global_lr = 0.00932, 1.7 #np.sqrt(K)\n",
    "\n",
    "_, centers_data_loaders, all_test_loader, _ = dataPreprocessing(isles_data_root, modality, 4, 2)\n",
    "\n",
    "#move center 3 at the end of the dataloaders\n",
    "tmp = centers_data_loaders[2]\n",
    "centers_data_loaders[2]=centers_data_loaders[3]\n",
    "centers_data_loaders[3]=tmp\n",
    "\n",
    "options = {'K': K, 'l_epoch': local_epoch, 'B': batch_size, 'g_epoch': global_epoch, 'clients': clients,\n",
    "           'l_lr':local_lr, 'g_lr':global_lr, 'dataloader':centers_data_loaders, 'suffix': 'FedRod', \n",
    "           'scaffold_controls': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23b46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network present in each client\n",
    "class UNet_custom(monai.networks.nets.UNet):\n",
    "    def __init__(self, spatial_dims, in_channels, out_channels, channels,\n",
    "                 strides, kernel_size, num_res_units, name, scaff=False, fed_rod=False):\n",
    "        #call parent constructor\n",
    "        super(UNet_custom, self).__init__(spatial_dims=spatial_dims,\n",
    "                                          in_channels=in_channels,\n",
    "                                          out_channels=out_channels, \n",
    "                                          channels=channels,\n",
    "                                          strides=strides,\n",
    "                                          kernel_size=kernel_size, \n",
    "                                          num_res_units=num_res_units)\n",
    "\n",
    "        self.name = name\n",
    "        self.control = {}\n",
    "        self.delta_control = {}\n",
    "        self.delta_y = {}\n",
    "        if fed_rod:\n",
    "            #Unet params sets for FedRod\n",
    "            self.encoder_generic = {}\n",
    "            self.decoder_generic = {}\n",
    "            self.decoder_personalized = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22419a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedRod(Fedem):\n",
    "    def __init__(self, options):\n",
    "        super(FedRod, self).__init__(options)\n",
    "        self.writer = SummaryWriter(f\"runs/llr{options['l_lr']}_glr{options['g_lr']}_le{options['l_epoch']}_ge{options['g_epoch']}_{options['K']}sites_\"+\"FEDROD\"+options['suffix'])\n",
    "        self.K = options['K']\n",
    "        self.name_encoder_layers = [\"model.0\", \"model.1.submodule.0\", \"model.1.submodule.1.submodule.2.0\",\n",
    "                                    \"model.1.submodule.1.submodule.0\", \"model.1.submodule.1.submodule.1\"]\n",
    "        \n",
    "        self.name_decoder_layers  = ['model.1.submodule.1.submodule.2.1',\n",
    "                                    'model.1.submodule.2', 'model.2']\n",
    "\n",
    "        #server model\n",
    "        self.nn = UNet_custom(spatial_dims=2,\n",
    "                             in_channels=1,\n",
    "                             out_channels=1,\n",
    "                             channels=(16, 32, 64, 128),\n",
    "                             strides=(2, 2, 2),\n",
    "                             kernel_size = (3,3),\n",
    "                             num_res_units=2,\n",
    "                             name='server',\n",
    "                             scaff=False,\n",
    "                             fed_rod=True).to(device)\n",
    "        \n",
    "        #Global encoder - decoder (inlcuding personalized) layers init\n",
    "        for k, v in self.nn.named_parameters():\n",
    "            for enc_layer in self.name_encoder_layers:\n",
    "                if enc_layer in k:\n",
    "                    self.nn.encoder_generic[k] = copy.deepcopy(v.data)\n",
    "            for dec_layer in self.name_decoder_layers:\n",
    "                if dec_layer in k:\n",
    "                    self.nn.decoder_generic[k] = copy.deepcopy(v.data)\n",
    "                    self.nn.decoder_personalized[k] = copy.deepcopy(v.data)\n",
    "                    \n",
    "        #print(self.nn.decoder_generic)\n",
    "        #clients of the federation\n",
    "        self.nns = []\n",
    "        for i in range(len(options['clients'])):\n",
    "            temp = copy.deepcopy(self.nn)\n",
    "            temp.name = options['clients'][i]\n",
    "            temp.encoder_generic = copy.deepcopy(self.nn.encoder_generic)\n",
    "            temp.decoder_generic = copy.deepcopy(self.nn.decoder_generic)\n",
    "            temp.decoder_personalized = copy.deepcopy(self.nn.decoder_personalized)            \n",
    "            self.nns.append(temp)\n",
    "            \n",
    "    \n",
    "\n",
    "    def aggregation(self, index, global_lr, **kwargs):\n",
    "        s = 0.0\n",
    "        for j in index:\n",
    "            # normal\n",
    "            s += self.nns[j].len\n",
    "                \n",
    "        # Agregating the generic encoder from clients encoders\n",
    "        for j in index:\n",
    "            for k, v in self.nn.named_parameters():\n",
    "                for enc_layer in self.name_encoder_layers:\n",
    "                    if enc_layer in k:\n",
    "                        v.data += self.nns[j].encoder_generic[k]  / len(index) #check other weightings here\n",
    "\n",
    "        # Agregating the generic decoder from clients decoders\n",
    "        for j in index:\n",
    "            for k, v in self.nn.named_parameters():\n",
    "                for dec_layer in self.name_decoder_layers:\n",
    "                    if dec_layer in k:\n",
    "                        v.data += self.nns[j].decoder_generic[k]  / len(index)  #check other weightings here\n",
    "\n",
    "\n",
    "    def train(self, ann, dataloader_train, local_epoch, local_lr):\n",
    "        #First the generic encoder-decoder are updated       \n",
    "        ann.train()\n",
    "        ann.len = len(dataloader_train)\n",
    "                \n",
    "        x = copy.deepcopy(ann)\n",
    "        loss_function = monai.losses.DiceLoss(sigmoid=True,include_background=False)\n",
    "        #One option is to set here the weights to 0 before the optimizer receives the parametets\n",
    "        optimizer = torch.optim.Adam(ann.parameters(), lr=local_lr)\n",
    "\n",
    "        for epoch in range(local_epoch):\n",
    "            for batch_data in dataloader_train:\n",
    "                #(1)Optimization of the Generic path here equation (8) of the paper\n",
    "                for k, v in ann.named_parameters(): #Transfering data from the generic head\n",
    "                    for dec_layer in self.name_decoder_layers:\n",
    "                        if dec_layer in k:\n",
    "                            v.data = copy.deepcopy(ann.decoder_generic[k]) #\"Swapping the heads\"\n",
    "                    v.requires_grad = True #deriving gradients to all the generic layers\n",
    "                \n",
    "                inputs, labels = batch_data[0][:,:,:,:,0].to(device), batch_data[1][:,:,:,:,0].to(device)\n",
    "                y_pred_generic = ann(inputs)\n",
    "                loss = loss_function(y_pred_generic, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #print(\"=====================================================\")\n",
    "                #for k, v in ann.named_parameters():\n",
    "                #    print(v.requires_grad)\n",
    "                    \n",
    "                #(2)Optimization of the Perzonalized path here equation (9) of the paper\n",
    "                for k, v in ann.named_parameters():\n",
    "                    for dec_layer in self.name_decoder_layers:\n",
    "                        if dec_layer in k:\n",
    "                            ann.decoder_generic[k] = copy.deepcopy(v.data) #Keeping the generic decoder data\n",
    "                \n",
    "                #(3) Keeping the generic output to add it later to the personalized\n",
    "                output_generic = copy.deepcopy(y_pred_generic.detach().numpy())\n",
    "\n",
    "                for k,v in ann.named_parameters():\n",
    "                    for enc_layer_name in self.name_encoder_layers:\n",
    "                        if enc_layer_name in k:\n",
    "                            v.requires_grad = False\n",
    "\n",
    "                for k, v in ann.named_parameters():\n",
    "                    for dec_layer in self.name_decoder_layers:\n",
    "                        if dec_layer in k:\n",
    "                            v.data = copy.deepcopy(ann.decoder_personalized[k]) #\"Swapping the heads\"\n",
    "                            v.requires_grad = True #Deriving fradients only wrt to the personalized head\n",
    "               \n",
    "                #for k, v in ann.named_parameters():\n",
    "                #    print(v.requires_grad)\n",
    "\n",
    "                output_personalized = ann(inputs) + torch.tensor(output_generic) #regularized personalized output\n",
    "                loss = loss_function(output_personalized, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #print(\"=====================================================\")\n",
    "                                            \n",
    "        return ann, loss.item() \n",
    "\n",
    "    def global_test(self, aggreg_dataloader_test):\n",
    "        model = self.nn\n",
    "        model.eval()\n",
    "        \n",
    "        #test the global model on each individual dataloader\n",
    "        for k, client in enumerate(self.nns):\n",
    "            print(\"testing on\", client.name, \"dataloader\")\n",
    "            test(model, self.dataloaders[k][2])\n",
    "        \n",
    "        #test the global model on aggregated dataloaders\n",
    "        print(\"testing on all the data\")\n",
    "        test(model, aggreg_dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656e8835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fa3db6c3520>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fa3d0e98d00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fa3d0e86a00>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_data_loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4eca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_rod = FedRod(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c2dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fed/lib/python3.9/site-packages/monai/losses/dice.py:150: UserWarning: single channel prediction, `include_background=False` ignored.\n",
      "  warnings.warn(\"single channel prediction, `include_background=False` ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904084324836731\n"
     ]
    }
   ],
   "source": [
    "net, loss_center =  fed_rod.train(fed_rod.nns[0], fed_rod.dataloaders[0][0], 10, 0.0001)\n",
    "print(loss_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7c8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
