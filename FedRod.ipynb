{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ffc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac1e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from network import *\n",
    "from framework import *\n",
    "from preprocessing import dataPreprocessing\n",
    "##Networks\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6aef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "isles_data_root = '/str/data/ASAP/miccai22_data/isles/federated/'\n",
    "exp_root = '/home/otarola/miccai22/fedem/'\n",
    "modality = 'Tmax'\n",
    "batch_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc05c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients=[\"center1\", \"center2\", \"center4\"]\n",
    "#from SCAFFOLD manuscript, global_learning_rate should be = sqrt(#Samples sites)\n",
    "local_epoch, global_epoch = 2, 20\n",
    "#no sampling\n",
    "K=len(clients)\n",
    "\n",
    "local_lr, global_lr = 0.00932, 1.7 #np.sqrt(K)\n",
    "\n",
    "_, centers_data_loaders, all_test_loader, _ = dataPreprocessing(isles_data_root, modality, 4, 2)\n",
    "\n",
    "#move center 3 at the end of the dataloaders\n",
    "tmp = centers_data_loaders[2]\n",
    "centers_data_loaders[2]=centers_data_loaders[3]\n",
    "centers_data_loaders[3]=tmp\n",
    "\n",
    "options = {'K': K, 'l_epoch': local_epoch, 'B': batch_size, 'g_epoch': global_epoch, 'clients': clients,\n",
    "           'l_lr':local_lr, 'g_lr':global_lr, 'dataloader':centers_data_loaders, 'suffix': 'FedRod'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64baff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network present in each client\n",
    "class UNet_custom(monai.networks.nets.UNet):\n",
    "    def __init__(self, spatial_dims, in_channels, out_channels, channels,\n",
    "                 strides, kernel_size, num_res_units, name, scaff=False, fed_rod=False):\n",
    "        #call parent constructor\n",
    "        super(UNet_custom, self).__init__(spatial_dims=spatial_dims,\n",
    "                                          in_channels=in_channels,\n",
    "                                          out_channels=out_channels, \n",
    "                                          channels=channels,\n",
    "                                          strides=strides,\n",
    "                                          kernel_size=kernel_size, \n",
    "                                          num_res_units=num_res_units)\n",
    "\n",
    "        self.name = name\n",
    "        if scaff:\n",
    "            #control variables for SCAFFOLD\n",
    "            self.control = {}\n",
    "            self.delta_control = {}\n",
    "            self.delta_y = {}\n",
    "        if fed_rod:\n",
    "            #Unet partitions for FedRod\n",
    "            self.encoder = {}\n",
    "            self.decoder = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d052e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedRod(Fedem):\n",
    "    def __init__(self, options):\n",
    "        super(FedRod, self).__init__(options)\n",
    "        self.writer = SummaryWriter(f\"runs/llr{options['l_lr']}_glr{options['g_lr']}_le{options['l_epoch']}_ge{options['g_epoch']}_{options['K']}sites_\"+\"FEDROD\"+options['suffix'])\n",
    "        self.K = options['K']\n",
    "        \n",
    "        #server model\n",
    "        self.nn = UNet_custom(spatial_dims=2,\n",
    "                             in_channels=1,\n",
    "                             out_channels=1,\n",
    "                             channels=(16, 32, 64, 128),\n",
    "                             strides=(2, 2, 2),\n",
    "                             kernel_size = (3,3),\n",
    "                             num_res_units=2,\n",
    "                             name='server',\n",
    "                             scaff=False,\n",
    "                             fed_rod=True).to(device)\n",
    "    \n",
    "\n",
    "    def aggregation(self, index, global_lr, **kwargs):\n",
    "        s = 0.0\n",
    "        for j in index:\n",
    "            # normal\n",
    "            s += self.nns[j].len\n",
    "        # compute\n",
    "        x = {}\n",
    "        c = {}\n",
    "        # init\n",
    "        for k, v in self.nns[0].named_parameters():\n",
    "            x[k] = torch.zeros_like(v.data)\n",
    "            c[k] = torch.zeros_like(v.data)\n",
    "\n",
    "        for j in index:\n",
    "            for k, v in self.nns[j].named_parameters():\n",
    "                x[k] += self.nns[j].delta_y[k] / len(index)  # averaging\n",
    "                c[k] += self.nns[j].delta_control[k] / len(index)  # averaging\n",
    "\n",
    "        # update x and c\n",
    "        for k, v in self.nn.named_parameters():\n",
    "            v.data += x[k].data*global_lr\n",
    "            self.nn.control[k].data += c[k].data * (len(index) / self.K)\n",
    "\n",
    "    def train(self, ann, dataloader_train, local_epoch, local_lr):\n",
    "        #train client to train mode\n",
    "        ann.train()\n",
    "        ann.len = len(dataloader_train)\n",
    "                \n",
    "        x = copy.deepcopy(ann)\n",
    "        loss_function = monai.losses.DiceLoss(sigmoid=True,include_background=False)\n",
    "        optimizer = ScaffoldOptimizer(ann.parameters(), lr=local_lr, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(local_epoch):\n",
    "            for batch_data in dataloader_train:\n",
    "                inputs, labels = batch_data[0][:,:,:,:,0].to(device), batch_data[1][:,:,:,:,0].to(device)\n",
    "                y_pred = ann(inputs)\n",
    "                loss = loss_function(y_pred, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()          \n",
    "                optimizer.step(self.nn.control, ann.control) #performing SGD on the control variables\n",
    "                            \n",
    "        # update c\n",
    "        # c+ <- ci - c + 1/(E * lr) * (x-yi)\n",
    "        temp = {}\n",
    "        for k, v in ann.named_parameters():\n",
    "            temp[k] = v.data.clone()\n",
    "        for k, v in x.named_parameters():\n",
    "            ann.control[k] = ann.control[k] - self.nn.control[k] + (v.data - temp[k]) / (local_epoch * local_lr)\n",
    "            ann.delta_y[k] = temp[k] - v.data\n",
    "            ann.delta_control[k] = ann.control[k] - x.control[k]\n",
    "        return ann, loss.item()\n",
    "\n",
    "    def global_test(self, aggreg_dataloader_test):\n",
    "        model = self.nn\n",
    "        model.eval()\n",
    "        \n",
    "        #test the global model on each individual dataloader\n",
    "        for k, client in enumerate(self.nns):\n",
    "            print(\"testing on\", client.name, \"dataloader\")\n",
    "            test(model, self.dataloaders[k][2])\n",
    "        \n",
    "        #test the global model on aggregated dataloaders\n",
    "        print(\"testing on all the data\")\n",
    "        test(model, aggreg_dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5030ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_rod = FedRod(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddf1127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of UNet_custom(\n",
       "  (model): Sequential(\n",
       "    (0): ResidualUnit(\n",
       "      (conv): Sequential(\n",
       "        (unit0): Convolution(\n",
       "          (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "        (unit1): Convolution(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): SkipConnection(\n",
       "      (submodule): Sequential(\n",
       "        (0): ResidualUnit(\n",
       "          (conv): Sequential(\n",
       "            (unit0): Convolution(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (unit1): Convolution(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): SkipConnection(\n",
       "          (submodule): Sequential(\n",
       "            (0): ResidualUnit(\n",
       "              (conv): Sequential(\n",
       "                (unit0): Convolution(\n",
       "                  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "                (unit1): Convolution(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (residual): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "            (1): SkipConnection(\n",
       "              (submodule): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                  (unit1): Convolution(\n",
       "                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Convolution(\n",
       "                (conv): ConvTranspose2d(192, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Convolution(\n",
       "            (conv): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (adn): ADN(\n",
       "              (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              (D): Dropout(p=0.0, inplace=False)\n",
       "              (A): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (conv): Sequential(\n",
       "              (unit0): Convolution(\n",
       "                (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (residual): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualUnit(\n",
       "        (conv): Sequential(\n",
       "          (unit0): Convolution(\n",
       "            (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (residual): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed_rod.nn.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "903dfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = fed_rod.nn(torch.zeros([1, 1, 56, 224]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "699b8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph {\n",
      "\tgraph [size=\"66.0,66.0\"]\n",
      "\tnode [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]\n",
      "\t140132282293280 [label=\"\n",
      " ()\" fillcolor=darkolivegreen1]\n",
      "\t140136682582560 [label=MeanBackward0]\n",
      "\t140136682583472 -> 140136682582560\n",
      "\t140136682583472 [label=AddBackward0]\n",
      "\t140136682583280 -> 140136682583472\n",
      "\t140136682583280 [label=AddBackward0]\n",
      "\t140136682584576 -> 140136682583280\n",
      "\t140136682584576 [label=CudnnConvolutionBackward0]\n",
      "\t140136682584000 -> 140136682584576\n",
      "\t140136682584000 [label=PreluBackward0]\n",
      "\t140132282665280 -> 140136682584000\n",
      "\t140132282665280 [label=ViewBackward0]\n",
      "\t140132282666336 -> 140132282665280\n",
      "\t140132282666336 [label=NativeBatchNormBackward0]\n",
      "\t140132282666384 -> 140132282666336\n",
      "\t140132282666384 [label=ViewBackward0]\n",
      "\t140132282664800 -> 140132282666384\n",
      "\t140132282664800 [label=AddBackward0]\n",
      "\t140132282665664 -> 140132282664800\n",
      "\t140132282665664 [label=CudnnConvolutionTransposeBackward0]\n",
      "\t140132282665040 -> 140132282665664\n",
      "\t140132282665040 [label=CatBackward0]\n",
      "\t140132282665568 -> 140132282665040\n",
      "\t140132282665568 [label=AddBackward0]\n",
      "\t140132282666816 -> 140132282665568\n",
      "\t140132282666816 [label=PreluBackward0]\n",
      "\t140132282665808 -> 140132282666816\n",
      "\t140132282665808 [label=ViewBackward0]\n",
      "\t140132282666480 -> 140132282665808\n",
      "\t140132282666480 [label=NativeBatchNormBackward0]\n",
      "\t140132282161136 -> 140132282666480\n",
      "\t140132282161136 [label=ViewBackward0]\n",
      "\t140132282162864 -> 140132282161136\n",
      "\t140132282162864 [label=AddBackward0]\n",
      "\t140132282162912 -> 140132282162864\n",
      "\t140132282162912 [label=CudnnConvolutionBackward0]\n",
      "\t140132282163008 -> 140132282162912\n",
      "\t140132282163008 [label=PreluBackward0]\n",
      "\t140132282162768 -> 140132282163008\n",
      "\t140132282162768 [label=ViewBackward0]\n",
      "\t140132282160704 -> 140132282162768\n",
      "\t140132282160704 [label=NativeBatchNormBackward0]\n",
      "\t140132282160320 -> 140132282160704\n",
      "\t140132282160320 [label=ViewBackward0]\n",
      "\t140132282160464 -> 140132282160320\n",
      "\t140132282160464 [label=AddBackward0]\n",
      "\t140132282160416 -> 140132282160464\n",
      "\t140132282160416 [label=CudnnConvolutionBackward0]\n",
      "\t140132282160272 -> 140132282160416\n",
      "\t140132282291600 [label=\"model.0.conv.unit0.conv.weight\n",
      " (16, 1, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282291600 -> 140132282160272\n",
      "\t140132282160272 [label=AccumulateGrad]\n",
      "\t140132282160368 -> 140132282160464\n",
      "\t140132282160368 [label=ReshapeAliasBackward0]\n",
      "\t140132282159936 -> 140132282160368\n",
      "\t140132282291680 [label=\"model.0.conv.unit0.conv.bias\n",
      " (16)\" fillcolor=lightblue]\n",
      "\t140132282291680 -> 140132282159936\n",
      "\t140132282159936 [label=AccumulateGrad]\n",
      "\t140132282162672 -> 140132282163008\n",
      "\t140132282291520 [label=\"model.0.conv.unit0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282291520 -> 140132282162672\n",
      "\t140132282162672 [label=AccumulateGrad]\n",
      "\t140132282163104 -> 140132282162912\n",
      "\t140132282291200 [label=\"model.0.conv.unit1.conv.weight\n",
      " (16, 16, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282291200 -> 140132282163104\n",
      "\t140132282163104 [label=AccumulateGrad]\n",
      "\t140132282161280 -> 140132282162864\n",
      "\t140132282161280 [label=ReshapeAliasBackward0]\n",
      "\t140132282160512 -> 140132282161280\n",
      "\t140132282290480 [label=\"model.0.conv.unit1.conv.bias\n",
      " (16)\" fillcolor=lightblue]\n",
      "\t140132282290480 -> 140132282160512\n",
      "\t140132282160512 [label=AccumulateGrad]\n",
      "\t140132282665376 -> 140132282666816\n",
      "\t140132282290640 [label=\"model.0.conv.unit1.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282290640 -> 140132282665376\n",
      "\t140132282665376 [label=AccumulateGrad]\n",
      "\t140132282666528 -> 140132282665568\n",
      "\t140132282666528 [label=AddBackward0]\n",
      "\t140132282666288 -> 140132282666528\n",
      "\t140132282666288 [label=CudnnConvolutionBackward0]\n",
      "\t140132282160560 -> 140132282666288\n",
      "\t140132282290560 [label=\"model.0.residual.weight\n",
      " (16, 1, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282290560 -> 140132282160560\n",
      "\t140132282160560 [label=AccumulateGrad]\n",
      "\t140132282666672 -> 140132282666528\n",
      "\t140132282666672 [label=ReshapeAliasBackward0]\n",
      "\t140132282161088 -> 140132282666672\n",
      "\t140132282290400 [label=\"model.0.residual.bias\n",
      " (16)\" fillcolor=lightblue]\n",
      "\t140132282290400 -> 140132282161088\n",
      "\t140132282161088 [label=AccumulateGrad]\n",
      "\t140132282665328 -> 140132282665040\n",
      "\t140132282665328 [label=AddBackward0]\n",
      "\t140132282665520 -> 140132282665328\n",
      "\t140132282665520 [label=PreluBackward0]\n",
      "\t140132282162816 -> 140132282665520\n",
      "\t140132282162816 [label=ViewBackward0]\n",
      "\t140132282159264 -> 140132282162816\n",
      "\t140132282159264 [label=NativeBatchNormBackward0]\n",
      "\t140132282160608 -> 140132282159264\n",
      "\t140132282160608 [label=ViewBackward0]\n",
      "\t140132282159216 -> 140132282160608\n",
      "\t140132282159216 [label=AddBackward0]\n",
      "\t140132282159360 -> 140132282159216\n",
      "\t140132282159360 [label=CudnnConvolutionBackward0]\n",
      "\t140132282163056 -> 140132282159360\n",
      "\t140132282163056 [label=PreluBackward0]\n",
      "\t140132282160224 -> 140132282163056\n",
      "\t140132282160224 [label=ViewBackward0]\n",
      "\t140132282160128 -> 140132282160224\n",
      "\t140132282160128 [label=NativeBatchNormBackward0]\n",
      "\t140132282160032 -> 140132282160128\n",
      "\t140132282160032 [label=ViewBackward0]\n",
      "\t140132282160848 -> 140132282160032\n",
      "\t140132282160848 [label=AddBackward0]\n",
      "\t140132282160752 -> 140132282160848\n",
      "\t140132282160752 [label=CudnnConvolutionTransposeBackward0]\n",
      "\t140132282160944 -> 140132282160752\n",
      "\t140132282160944 [label=CatBackward0]\n",
      "\t140132282162336 -> 140132282160944\n",
      "\t140132282162336 [label=AddBackward0]\n",
      "\t140132282163152 -> 140132282162336\n",
      "\t140132282163152 [label=PreluBackward0]\n",
      "\t140132282162192 -> 140132282163152\n",
      "\t140132282162192 [label=ViewBackward0]\n",
      "\t140132282159552 -> 140132282162192\n",
      "\t140132282159552 [label=NativeBatchNormBackward0]\n",
      "\t140132282159744 -> 140132282159552\n",
      "\t140132282159744 [label=ViewBackward0]\n",
      "\t140132282161376 -> 140132282159744\n",
      "\t140132282161376 [label=AddBackward0]\n",
      "\t140132282162576 -> 140132282161376\n",
      "\t140132282162576 [label=CudnnConvolutionBackward0]\n",
      "\t140132282162432 -> 140132282162576\n",
      "\t140132282162432 [label=PreluBackward0]\n",
      "\t140132282162096 -> 140132282162432\n",
      "\t140132282162096 [label=ViewBackward0]\n",
      "\t140132282161904 -> 140132282162096\n",
      "\t140132282161904 [label=NativeBatchNormBackward0]\n",
      "\t140132282161760 -> 140132282161904\n",
      "\t140132282161760 [label=ViewBackward0]\n",
      "\t140132282161664 -> 140132282161760\n",
      "\t140132282161664 [label=AddBackward0]\n",
      "\t140132282161568 -> 140132282161664\n",
      "\t140132282161568 [label=CudnnConvolutionBackward0]\n",
      "\t140132282665568 -> 140132282161568\n",
      "\t140136682675888 -> 140132282161568\n",
      "\t140136682479344 [label=\"model.1.submodule.0.conv.unit0.conv.weight\n",
      " (32, 16, 3, 3)\" fillcolor=lightblue]\n",
      "\t140136682479344 -> 140136682675888\n",
      "\t140136682675888 [label=AccumulateGrad]\n",
      "\t140132282161616 -> 140132282161664\n",
      "\t140132282161616 [label=ReshapeAliasBackward0]\n",
      "\t140136682675312 -> 140132282161616\n",
      "\t140136682479264 [label=\"model.1.submodule.0.conv.unit0.conv.bias\n",
      " (32)\" fillcolor=lightblue]\n",
      "\t140136682479264 -> 140136682675312\n",
      "\t140136682675312 [label=AccumulateGrad]\n",
      "\t140132282162144 -> 140132282162432\n",
      "\t140136682479184 [label=\"model.1.submodule.0.conv.unit0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140136682479184 -> 140132282162144\n",
      "\t140132282162144 [label=AccumulateGrad]\n",
      "\t140132282162480 -> 140132282162576\n",
      "\t140132282293200 [label=\"model.1.submodule.0.conv.unit1.conv.weight\n",
      " (32, 32, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282293200 -> 140132282162480\n",
      "\t140132282162480 [label=AccumulateGrad]\n",
      "\t140132282162624 -> 140132282161376\n",
      "\t140132282162624 [label=ReshapeAliasBackward0]\n",
      "\t140132282161712 -> 140132282162624\n",
      "\t140132282292480 [label=\"model.1.submodule.0.conv.unit1.conv.bias\n",
      " (32)\" fillcolor=lightblue]\n",
      "\t140132282292480 -> 140132282161712\n",
      "\t140132282161712 [label=AccumulateGrad]\n",
      "\t140132282162240 -> 140132282163152\n",
      "\t140132282292560 [label=\"model.1.submodule.0.conv.unit1.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282292560 -> 140132282162240\n",
      "\t140132282162240 [label=AccumulateGrad]\n",
      "\t140132282161520 -> 140132282162336\n",
      "\t140132282161520 [label=AddBackward0]\n",
      "\t140132282159696 -> 140132282161520\n",
      "\t140132282159696 [label=CudnnConvolutionBackward0]\n",
      "\t140132282665568 -> 140132282159696\n",
      "\t140132282161856 -> 140132282159696\n",
      "\t140132282292640 [label=\"model.1.submodule.0.residual.weight\n",
      " (32, 16, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282292640 -> 140132282161856\n",
      "\t140132282161856 [label=AccumulateGrad]\n",
      "\t140132282161328 -> 140132282161520\n",
      "\t140132282161328 [label=ReshapeAliasBackward0]\n",
      "\t140132282162000 -> 140132282161328\n",
      "\t140132282290320 [label=\"model.1.submodule.0.residual.bias\n",
      " (32)\" fillcolor=lightblue]\n",
      "\t140132282290320 -> 140132282162000\n",
      "\t140132282162000 [label=AccumulateGrad]\n",
      "\t140132282161424 -> 140132282160944\n",
      "\t140132282161424 [label=AddBackward0]\n",
      "\t140132282162384 -> 140132282161424\n",
      "\t140132282162384 [label=PreluBackward0]\n",
      "\t140132282159312 -> 140132282162384\n",
      "\t140132282159312 [label=ViewBackward0]\n",
      "\t140132282162048 -> 140132282159312\n",
      "\t140132282162048 [label=NativeBatchNormBackward0]\n",
      "\t140136682674112 -> 140132282162048\n",
      "\t140136682674112 [label=ViewBackward0]\n",
      "\t140136682674496 -> 140136682674112\n",
      "\t140136682674496 [label=AddBackward0]\n",
      "\t140136682675024 -> 140136682674496\n",
      "\t140136682675024 [label=CudnnConvolutionBackward0]\n",
      "\t140132282162288 -> 140136682675024\n",
      "\t140132282162288 [label=PreluBackward0]\n",
      "\t140132273332432 -> 140132282162288\n",
      "\t140132273332432 [label=ViewBackward0]\n",
      "\t140132273332576 -> 140132273332432\n",
      "\t140132273332576 [label=NativeBatchNormBackward0]\n",
      "\t140132273332672 -> 140132273332576\n",
      "\t140132273332672 [label=ViewBackward0]\n",
      "\t140132273332768 -> 140132273332672\n",
      "\t140132273332768 [label=AddBackward0]\n",
      "\t140132273332864 -> 140132273332768\n",
      "\t140132273332864 [label=CudnnConvolutionTransposeBackward0]\n",
      "\t140132273333008 -> 140132273332864\n",
      "\t140132273333008 [label=CatBackward0]\n",
      "\t140132273333152 -> 140132273333008\n",
      "\t140132273333152 [label=AddBackward0]\n",
      "\t140132273333296 -> 140132273333152\n",
      "\t140132273333296 [label=PreluBackward0]\n",
      "\t140132273333440 -> 140132273333296\n",
      "\t140132273333440 [label=ViewBackward0]\n",
      "\t140132273333584 -> 140132273333440\n",
      "\t140132273333584 [label=NativeBatchNormBackward0]\n",
      "\t140132273333680 -> 140132273333584\n",
      "\t140132273333680 [label=ViewBackward0]\n",
      "\t140132273333776 -> 140132273333680\n",
      "\t140132273333776 [label=AddBackward0]\n",
      "\t140132273333872 -> 140132273333776\n",
      "\t140132273333872 [label=CudnnConvolutionBackward0]\n",
      "\t140132273334016 -> 140132273333872\n",
      "\t140132273334016 [label=PreluBackward0]\n",
      "\t140132273334160 -> 140132273334016\n",
      "\t140132273334160 [label=ViewBackward0]\n",
      "\t140132273334304 -> 140132273334160\n",
      "\t140132273334304 [label=NativeBatchNormBackward0]\n",
      "\t140132273334400 -> 140132273334304\n",
      "\t140132273334400 [label=ViewBackward0]\n",
      "\t140132273334496 -> 140132273334400\n",
      "\t140132273334496 [label=AddBackward0]\n",
      "\t140132273334592 -> 140132273334496\n",
      "\t140132273334592 [label=CudnnConvolutionBackward0]\n",
      "\t140132282162336 -> 140132273334592\n",
      "\t140132273334736 -> 140132273334592\n",
      "\t140132452124368 [label=\"model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      " (64, 32, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132452124368 -> 140132273334736\n",
      "\t140132273334736 [label=AccumulateGrad]\n",
      "\t140132273334544 -> 140132273334496\n",
      "\t140132273334544 [label=ReshapeAliasBackward0]\n",
      "\t140132273334784 -> 140132273334544\n",
      "\t140132452124608 [label=\"model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      " (64)\" fillcolor=lightblue]\n",
      "\t140132452124608 -> 140132273334784\n",
      "\t140132273334784 [label=AccumulateGrad]\n",
      "\t140132273334112 -> 140132273334016\n",
      "\t140132452124048 [label=\"model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132452124048 -> 140132273334112\n",
      "\t140132273334112 [label=AccumulateGrad]\n",
      "\t140132273333968 -> 140132273333872\n",
      "\t140132452123488 [label=\"model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      " (64, 64, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132452123488 -> 140132273333968\n",
      "\t140132273333968 [label=AccumulateGrad]\n",
      "\t140132273333824 -> 140132273333776\n",
      "\t140132273333824 [label=ReshapeAliasBackward0]\n",
      "\t140132273334448 -> 140132273333824\n",
      "\t140132452123328 [label=\"model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      " (64)\" fillcolor=lightblue]\n",
      "\t140132452123328 -> 140132273334448\n",
      "\t140132273334448 [label=AccumulateGrad]\n",
      "\t140132273333392 -> 140132273333296\n",
      "\t140132452123168 [label=\"model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132452123168 -> 140132273333392\n",
      "\t140132273333392 [label=AccumulateGrad]\n",
      "\t140132273333248 -> 140132273333152\n",
      "\t140132273333248 [label=AddBackward0]\n",
      "\t140132273333632 -> 140132273333248\n",
      "\t140132273333632 [label=CudnnConvolutionBackward0]\n",
      "\t140132282162336 -> 140132273333632\n",
      "\t140132273334352 -> 140132273333632\n",
      "\t140132452123088 [label=\"model.1.submodule.1.submodule.0.residual.weight\n",
      " (64, 32, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132452123088 -> 140132273334352\n",
      "\t140132273334352 [label=AccumulateGrad]\n",
      "\t140132273333728 -> 140132273333248\n",
      "\t140132273333728 [label=ReshapeAliasBackward0]\n",
      "\t140132273334256 -> 140132273333728\n",
      "\t140132452123248 [label=\"model.1.submodule.1.submodule.0.residual.bias\n",
      " (64)\" fillcolor=lightblue]\n",
      "\t140132452123248 -> 140132273334256\n",
      "\t140132273334256 [label=AccumulateGrad]\n",
      "\t140132273333104 -> 140132273333008\n",
      "\t140132273333104 [label=AddBackward0]\n",
      "\t140132273334064 -> 140132273333104\n",
      "\t140132273334064 [label=PreluBackward0]\n",
      "\t140132273333488 -> 140132273334064\n",
      "\t140132273333488 [label=ViewBackward0]\n",
      "\t140132273334880 -> 140132273333488\n",
      "\t140132273334880 [label=NativeBatchNormBackward0]\n",
      "\t140132273334688 -> 140132273334880\n",
      "\t140132273334688 [label=ViewBackward0]\n",
      "\t140132273334976 -> 140132273334688\n",
      "\t140132273334976 [label=AddBackward0]\n",
      "\t140132273335072 -> 140132273334976\n",
      "\t140132273335072 [label=CudnnConvolutionBackward0]\n",
      "\t140132273335216 -> 140132273335072\n",
      "\t140132273335216 [label=PreluBackward0]\n",
      "\t140132273335360 -> 140132273335216\n",
      "\t140132273335360 [label=ViewBackward0]\n",
      "\t140132273335504 -> 140132273335360\n",
      "\t140132273335504 [label=NativeBatchNormBackward0]\n",
      "\t140132273335600 -> 140132273335504\n",
      "\t140132273335600 [label=ViewBackward0]\n",
      "\t140132273335696 -> 140132273335600\n",
      "\t140132273335696 [label=AddBackward0]\n",
      "\t140132273335792 -> 140132273335696\n",
      "\t140132273335792 [label=CudnnConvolutionBackward0]\n",
      "\t140132273333152 -> 140132273335792\n",
      "\t140132273335936 -> 140132273335792\n",
      "\t140132452125168 [label=\"model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      " (128, 64, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132452125168 -> 140132273335936\n",
      "\t140132273335936 [label=AccumulateGrad]\n",
      "\t140132273335744 -> 140132273335696\n",
      "\t140132273335744 [label=ReshapeAliasBackward0]\n",
      "\t140132273335984 -> 140132273335744\n",
      "\t140132452125088 [label=\"model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      " (128)\" fillcolor=lightblue]\n",
      "\t140132452125088 -> 140132273335984\n",
      "\t140132273335984 [label=AccumulateGrad]\n",
      "\t140132273335312 -> 140132273335216\n",
      "\t140136682604720 [label=\"model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140136682604720 -> 140132273335312\n",
      "\t140132273335312 [label=AccumulateGrad]\n",
      "\t140132273335168 -> 140132273335072\n",
      "\t140132452125408 [label=\"model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      " (128, 128, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132452125408 -> 140132273335168\n",
      "\t140132273335168 [label=AccumulateGrad]\n",
      "\t140132273335024 -> 140132273334976\n",
      "\t140132273335024 [label=ReshapeAliasBackward0]\n",
      "\t140132273335648 -> 140132273335024\n",
      "\t140132452125648 [label=\"model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      " (128)\" fillcolor=lightblue]\n",
      "\t140132452125648 -> 140132273335648\n",
      "\t140132273335648 [label=AccumulateGrad]\n",
      "\t140132273333536 -> 140132273334064\n",
      "\t140132452125808 [label=\"model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132452125808 -> 140132273333536\n",
      "\t140132273333536 [label=AccumulateGrad]\n",
      "\t140132273333344 -> 140132273333104\n",
      "\t140132273333344 [label=AddBackward0]\n",
      "\t140132273334640 -> 140132273333344\n",
      "\t140132273334640 [label=CudnnConvolutionBackward0]\n",
      "\t140132273333152 -> 140132273334640\n",
      "\t140132273335552 -> 140132273334640\n",
      "\t140132452126048 [label=\"model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      " (128, 64, 1, 1)\" fillcolor=lightblue]\n",
      "\t140132452126048 -> 140132273335552\n",
      "\t140132273335552 [label=AccumulateGrad]\n",
      "\t140132273334928 -> 140132273333344\n",
      "\t140132273334928 [label=ReshapeAliasBackward0]\n",
      "\t140132273335456 -> 140132273334928\n",
      "\t140132452124848 [label=\"model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      " (128)\" fillcolor=lightblue]\n",
      "\t140132452124848 -> 140132273335456\n",
      "\t140132273335456 [label=AccumulateGrad]\n",
      "\t140132273332960 -> 140132273332864\n",
      "\t140132452124208 [label=\"model.1.submodule.1.submodule.2.0.conv.weight\n",
      " (192, 32, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132452124208 -> 140132273332960\n",
      "\t140132273332960 [label=AccumulateGrad]\n",
      "\t140132273332816 -> 140132273332768\n",
      "\t140132273332816 [label=ReshapeAliasBackward0]\n",
      "\t140132273335120 -> 140132273332816\n",
      "\t140132452123648 [label=\"model.1.submodule.1.submodule.2.0.conv.bias\n",
      " (32)\" fillcolor=lightblue]\n",
      "\t140132452123648 -> 140132273335120\n",
      "\t140132273335120 [label=AccumulateGrad]\n",
      "\t140132273332384 -> 140132282162288\n",
      "\t140132452123728 [label=\"model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132452123728 -> 140132273332384\n",
      "\t140132273332384 [label=AccumulateGrad]\n",
      "\t140136682674544 -> 140136682675024\n",
      "\t140132452126448 [label=\"model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      " (32, 32, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132452126448 -> 140136682674544\n",
      "\t140136682674544 [label=AccumulateGrad]\n",
      "\t140136682674976 -> 140136682674496\n",
      "\t140136682674976 [label=ReshapeAliasBackward0]\n",
      "\t140132273332720 -> 140136682674976\n",
      "\t140132452126608 [label=\"model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      " (32)\" fillcolor=lightblue]\n",
      "\t140132452126608 -> 140132273332720\n",
      "\t140132273332720 [label=AccumulateGrad]\n",
      "\t140132282159408 -> 140132282162384\n",
      "\t140136682392688 [label=\"model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140136682392688 -> 140132282159408\n",
      "\t140132282159408 [label=AccumulateGrad]\n",
      "\t140132282162288 -> 140132282161424\n",
      "\t140132282160992 -> 140132282160752\n",
      "\t140132282290880 [label=\"model.1.submodule.2.0.conv.weight\n",
      " (64, 16, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282290880 -> 140132282160992\n",
      "\t140132282160992 [label=AccumulateGrad]\n",
      "\t140132282160800 -> 140132282160848\n",
      "\t140132282160800 [label=ReshapeAliasBackward0]\n",
      "\t140132282161472 -> 140132282160800\n",
      "\t140132282291040 [label=\"model.1.submodule.2.0.conv.bias\n",
      " (16)\" fillcolor=lightblue]\n",
      "\t140132282291040 -> 140132282161472\n",
      "\t140132282161472 [label=AccumulateGrad]\n",
      "\t140132282159792 -> 140132282163056\n",
      "\t140132282291120 [label=\"model.1.submodule.2.0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282291120 -> 140132282159792\n",
      "\t140132282159792 [label=AccumulateGrad]\n",
      "\t140132282159168 -> 140132282159360\n",
      "\t140132282291280 [label=\"model.1.submodule.2.1.conv.unit0.conv.weight\n",
      " (16, 16, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282291280 -> 140132282159168\n",
      "\t140132282159168 [label=AccumulateGrad]\n",
      "\t140132282159456 -> 140132282159216\n",
      "\t140132282159456 [label=ReshapeAliasBackward0]\n",
      "\t140132282160896 -> 140132282159456\n",
      "\t140132282291360 [label=\"model.1.submodule.2.1.conv.unit0.conv.bias\n",
      " (16)\" fillcolor=lightblue]\n",
      "\t140132282291360 -> 140132282160896\n",
      "\t140132282160896 [label=AccumulateGrad]\n",
      "\t140132282161232 -> 140132282665520\n",
      "\t140132282291760 [label=\"model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282291760 -> 140132282161232\n",
      "\t140132282161232 [label=AccumulateGrad]\n",
      "\t140132282163056 -> 140132282665328\n",
      "\t140132282665424 -> 140132282665664\n",
      "\t140132282292000 [label=\"model.2.0.conv.weight\n",
      " (32, 1, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282292000 -> 140132282665424\n",
      "\t140132282665424 [label=AccumulateGrad]\n",
      "\t140132282665712 -> 140132282664800\n",
      "\t140132282665712 [label=ReshapeAliasBackward0]\n",
      "\t140132282665232 -> 140132282665712\n",
      "\t140132282292080 [label=\"model.2.0.conv.bias\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282292080 -> 140132282665232\n",
      "\t140132282665232 [label=AccumulateGrad]\n",
      "\t140132282665952 -> 140136682584000\n",
      "\t140132282292240 [label=\"model.2.0.adn.A.weight\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282292240 -> 140132282665952\n",
      "\t140132282665952 [label=AccumulateGrad]\n",
      "\t140136682585056 -> 140136682584576\n",
      "\t140132282292400 [label=\"model.2.1.conv.unit0.conv.weight\n",
      " (1, 1, 3, 3)\" fillcolor=lightblue]\n",
      "\t140132282292400 -> 140136682585056\n",
      "\t140136682585056 [label=AccumulateGrad]\n",
      "\t140136682582944 -> 140136682583280\n",
      "\t140136682582944 [label=ReshapeAliasBackward0]\n",
      "\t140132282665760 -> 140136682582944\n",
      "\t140132282292160 [label=\"model.2.1.conv.unit0.conv.bias\n",
      " (1)\" fillcolor=lightblue]\n",
      "\t140132282292160 -> 140132282665760\n",
      "\t140132282665760 [label=AccumulateGrad]\n",
      "\t140136682584000 -> 140136682583472\n",
      "\t140136682582560 -> 140132282293280\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'filename.png'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot=make_dot(yhat.mean(), params=dict(fed_rod.nn.named_parameters()))\n",
    "print(dot)\n",
    "dot.format = 'png'\n",
    "dot.render(\"filename\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8eb9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.build_graph??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83458600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in fed_rod.nn.named_parameters():\n",
    "#    if \"model.1.submodule.1.submodule.1\" in k or \"model.0\" in k:\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "700fd65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "name_encoder_layers = [\"model.0\", \"model.1.submodule.0\", \"model.1.submodule.1.submodule.0\",\"model.1.submodule.1.submodule.1\"]\n",
    "\n",
    "for k,v in fed_rod.nn.named_parameters():\n",
    "    for enc_layer_name in name_encoder_layers:\n",
    "        if enc_layer_name in k:\n",
    "            print(k)\n",
    "            print(v.shape)\n",
    "            print(\"=====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c4b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = copy.deepcopy(fed_rod.nn)\n",
    "model2 = copy.deepcopy(fed_rod.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48c2b2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet_custom(\n",
       "  (model): Sequential(\n",
       "    (0): ResidualUnit(\n",
       "      (conv): Sequential(\n",
       "        (unit0): Convolution(\n",
       "          (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "        (unit1): Convolution(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): SkipConnection(\n",
       "      (submodule): Sequential(\n",
       "        (0): ResidualUnit(\n",
       "          (conv): Sequential(\n",
       "            (unit0): Convolution(\n",
       "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (unit1): Convolution(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (residual): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): SkipConnection(\n",
       "          (submodule): Sequential(\n",
       "            (0): ResidualUnit(\n",
       "              (conv): Sequential(\n",
       "                (unit0): Convolution(\n",
       "                  (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "                (unit1): Convolution(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (residual): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "            (1): SkipConnection(\n",
       "              (submodule): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                  (unit1): Convolution(\n",
       "                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Convolution(\n",
       "                (conv): ConvTranspose2d(192, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (conv): Sequential(\n",
       "                  (unit0): Convolution(\n",
       "                    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (residual): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Convolution(\n",
       "            (conv): ConvTranspose2d(64, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "            (adn): ADN(\n",
       "              (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "              (D): Dropout(p=0.0, inplace=False)\n",
       "              (A): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (conv): Sequential(\n",
       "              (unit0): Convolution(\n",
       "                (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (adn): ADN(\n",
       "                  (N): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                  (D): Dropout(p=0.0, inplace=False)\n",
       "                  (A): PReLU(num_parameters=1)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (residual): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (adn): ADN(\n",
       "          (N): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (D): Dropout(p=0.0, inplace=False)\n",
       "          (A): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualUnit(\n",
       "        (conv): Sequential(\n",
       "          (unit0): Convolution(\n",
       "            (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (residual): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed_rod.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e42853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.1.submodule.1.submodule.1'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c79b7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in model1.named_parameters():\n",
    "    for enc_layer_name in name_encoder_layers:\n",
    "        if enc_layer_name in k:\n",
    "            v.data = torch.zeros(v.shape).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b5607cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(-3.2642, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.1007, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(-0.2211, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.3308, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(-3.0522, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(1.4100, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.2066, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(-4.1414, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(-2.0412, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "tensor(-0.2770, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(-4.0785, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(-0.0989, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "tensor(1.5437, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(-0.0189, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(1.7056, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "tensor(0.2725, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "tensor(-5.6683, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(-0.3172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "tensor(-5.6952, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0.0336, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "tensor(3.7879, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "tensor(-0.1436, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "tensor(4.2015, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.1056, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(-2.3031, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.1495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "tensor(-1.3420, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.2681, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(-0.6828, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "tensor(2.5708, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(0.2897, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor(0.8217, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(-0.1656, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in fed_rod.nn.named_parameters():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(v.sum())    \n",
    "    print(\"=====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e0d59fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "tensor(4.2015, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.1056, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(-2.3031, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.1495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "tensor(-1.3420, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.2681, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(-0.6828, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "tensor(2.5708, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(0.2897, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor(0.8217, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(-0.1656, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in model1.named_parameters():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(v.sum())    \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9e0e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(-3.2642, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.1007, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(-0.2211, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(-0.3308, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.weight\n",
      "torch.Size([16, 1, 3, 3])\n",
      "tensor(-3.0522, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.0.residual.bias\n",
      "torch.Size([16])\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(1.4100, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.2066, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(-4.1414, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.weight\n",
      "torch.Size([32, 16, 3, 3])\n",
      "tensor(-2.0412, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.0.residual.bias\n",
      "torch.Size([32])\n",
      "tensor(-0.2770, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(-4.0785, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(-0.0989, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "tensor(1.5437, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "torch.Size([64])\n",
      "tensor(-0.0189, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "tensor(1.7056, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.0.residual.bias\n",
      "torch.Size([64])\n",
      "tensor(0.2725, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "tensor(-5.6683, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(-0.3172, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "tensor(-5.6952, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "torch.Size([128])\n",
      "tensor(0.0336, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "tensor(3.7879, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.1.submodule.residual.bias\n",
      "torch.Size([128])\n",
      "tensor(-0.1436, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "torch.Size([192, 32, 3, 3])\n",
      "tensor(4.2015, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.1056, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([32, 32, 3, 3])\n",
      "tensor(-2.3031, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([32])\n",
      "tensor(0.1495, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.weight\n",
      "torch.Size([64, 16, 3, 3])\n",
      "tensor(-1.3420, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.2681, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "torch.Size([16, 16, 3, 3])\n",
      "tensor(-0.6828, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "torch.Size([16])\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.weight\n",
      "torch.Size([32, 1, 3, 3])\n",
      "tensor(2.5708, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(0.2897, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.0.adn.A.weight\n",
      "torch.Size([1])\n",
      "tensor(0.2500, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor(0.8217, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n",
      "model.2.1.conv.unit0.conv.bias\n",
      "torch.Size([1])\n",
      "tensor(-0.1656, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k,v in model2.named_parameters():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print(v.sum())    \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553725e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
